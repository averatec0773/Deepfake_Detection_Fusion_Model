{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOL3Ha3ThpSoE1QuovsKVsQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/averatec0773/Deepfake_Detection_Fusion_Model/blob/main/Fusion_Model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "6dMqNxO-B1xB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b6f94a19-0276-4349-955b-ea94695212d1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import cv2\n",
        "from pathlib import Path\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tarfile\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Extract Dataset\n",
        "tmp_dir = \"/content/tmp_extract\"\n",
        "os.makedirs(tmp_dir, exist_ok=True)\n",
        "\n",
        "tar_path = \"/content/drive/MyDrive/Colab Notebooks/EECE7370/exported_data.tar.gz\"\n",
        "dataset_path = \"/content/drive/MyDrive/Colab Notebooks/EECE7370/\"\n",
        "\n",
        "with tarfile.open(tar_path, \"r:gz\") as tar:\n",
        "    tar.extractall(path=tmp_dir, filter=\"data\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ZUrRhwvJvt4",
        "outputId": "bb70c33c-c249-49a6-93f6-8320c4df0bcb"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# MOUTH CONFIG\n",
        "REAL_MOUTH_DIR = \"/content/tmp_extract/Features/original/mouth\"\n",
        "FAKE_MOUTH_DIR = \"/content/tmp_extract/Features/manipulated/mouth\"\n",
        "\n",
        "MOUTH_MODEL_WEIGHTS = \"/content/drive/MyDrive/Colab Notebooks/EECE7370/Model_mouth/CNN_Mouth_best_1207_0909.pth\"\n",
        "MOUTH_IMG_SIZE = 64\n",
        "MOPUTH_BATCH_SIZE = 64\n",
        "\n",
        "# EYES CONFIG\n",
        "EYES_DATA_ROOT = \"/content/tmp_extract\"\n",
        "EYES_MODEL_WEIGHTS = \"/content/drive/MyDrive/Colab Notebooks/EECE7370/Model_eyes_nose/model_a_eyes_best.pth\"\n",
        "\n",
        "EYES_IMG_SIZE = 50\n",
        "EYES_BATCH_SIZE = 32\n",
        "\n",
        "# FACE CONFIG\n",
        "REAL_FACE_DIR = \"/content/tmp_extract/CroppedFaces/original\"\n",
        "FAKE_FACE_DIR = \"/content/tmp_extract/CroppedFaces/manipulated\"\n",
        "\n",
        "FACE_MODEL_WEIGHTS = \"/content/drive/MyDrive/Colab Notebooks/EECE7370/Model_face/paper_model_c_best.pth\"\n",
        "\n",
        "FACE_IMG_SIZE = 224\n",
        "FACE_BATCH_SIZE = 64\n"
      ],
      "metadata": {
        "id": "0r0C3vLqFtxP"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# MOUTH DATASET & DATALOADER\n",
        "\n",
        "def mouth_image_transform(img, img_size=64):\n",
        "    img = img.resize((img_size, img_size))\n",
        "    arr = np.array(img).astype(np.float32) / 255.0\n",
        "    arr = np.transpose(arr, (2, 0, 1))  # HWC -> CHW\n",
        "    return torch.from_numpy(arr)\n",
        "\n",
        "class Mouth_TransformWrapper:\n",
        "    def __init__(self, img_size=64):\n",
        "        self.img_size = img_size\n",
        "\n",
        "    def __call__(self, img):\n",
        "        return mouth_image_transform(img, self.img_size)\n",
        "\n",
        "\n",
        "class MouthDataset(Dataset):\n",
        "    def __init__(self, real_dir, fake_dir, transform=None):\n",
        "        self.samples = []\n",
        "        self.transform = transform\n",
        "\n",
        "        real_dir = Path(real_dir)\n",
        "        fake_dir = Path(fake_dir)\n",
        "\n",
        "        # real label = 0\n",
        "        for ext in [\"*.jpg\", \"*.png\", \"*.jpeg\"]:\n",
        "            for p in real_dir.glob(ext):\n",
        "                self.samples.append((p, 0))\n",
        "\n",
        "        # fake label = 1\n",
        "        for ext in [\"*.jpg\", \"*.png\", \"*.jpeg\"]:\n",
        "            for p in fake_dir.glob(ext):\n",
        "                self.samples.append((p, 1))\n",
        "\n",
        "        # Sort samples by path name for deterministic order\n",
        "        self.samples.sort(key=lambda x: str(x[0]))\n",
        "        print(f\"[MouthDataset] Total samples: {len(self.samples)}\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path, label = self.samples[idx]\n",
        "        img = Image.open(img_path).convert(\"RGB\")\n",
        "\n",
        "        if self.transform is not None:\n",
        "            img = self.transform(img)\n",
        "\n",
        "        label = torch.tensor(label, dtype=torch.long)\n",
        "        return img, label, str(img_path)\n",
        "\n",
        "\n",
        "def create_mouth_dataloader(\n",
        "    real_dir,\n",
        "    fake_dir,\n",
        "    img_size=64,\n",
        "    batch_size=64,\n",
        "    shuffle=False,\n",
        "    num_workers=2,\n",
        "    pin_memory=True,\n",
        "):\n",
        "    transform = Mouth_TransformWrapper(img_size=img_size)\n",
        "    dataset = MouthDataset(real_dir, fake_dir, transform=transform)\n",
        "\n",
        "    loader = DataLoader(\n",
        "        dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=shuffle,\n",
        "        num_workers=num_workers,\n",
        "        pin_memory=pin_memory,\n",
        "    )\n",
        "    return dataset, loader"
      ],
      "metadata": {
        "id": "ax36elziFx4E"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# EYES DATASET & DATALOADER\n",
        "\n",
        "def eyes_image_transform(img_bgr, img_size=50):\n",
        "    \"\"\"Resize and normalize eye region using OpenCV.\"\"\"\n",
        "    img_rgb = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2RGB)\n",
        "    img_resized = cv2.resize(img_rgb, (img_size, img_size))\n",
        "    img_resized = img_resized.astype(np.float32) / 255.0\n",
        "    img_tensor = torch.from_numpy(img_resized).permute(2, 0, 1)  # (H, W, C) -> (C, H, W)\n",
        "    return img_tensor\n",
        "\n",
        "\n",
        "class EyesDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Dataset for eyes region (leftEye + rightEye).\n",
        "    Returns (img_tensor, label_tensor, path_str).\n",
        "    \"\"\"\n",
        "    def __init__(self, data_root, img_size=50):\n",
        "        self.img_size = img_size\n",
        "        self.file_paths = []\n",
        "        self.labels = []\n",
        "\n",
        "        # Equivalent to get_file_paths(data_root, region='eyes')\n",
        "        regions = [\"leftEye\", \"rightEye\"]\n",
        "\n",
        "        for r in regions:\n",
        "            # Original (real) - label 0\n",
        "            folder_real = os.path.join(data_root, \"Features\", \"original\", r)\n",
        "            if os.path.exists(folder_real):\n",
        "                for filename in os.listdir(folder_real):\n",
        "                    self.file_paths.append(os.path.join(folder_real, filename))\n",
        "                    self.labels.append(0)\n",
        "\n",
        "            # Manipulated (fake) - label 1\n",
        "            folder_fake = os.path.join(data_root, \"Features\", \"manipulated\", r)\n",
        "            if os.path.exists(folder_fake):\n",
        "                for filename in os.listdir(folder_fake):\n",
        "                    self.file_paths.append(os.path.join(folder_fake, filename))\n",
        "                    self.labels.append(1)\n",
        "\n",
        "        # Sort by path for deterministic order\n",
        "        paired = list(zip(self.file_paths, self.labels))\n",
        "        paired.sort(key=lambda x: x[0])\n",
        "        self.file_paths = [p for p, _ in paired]\n",
        "        self.labels = [l for _, l in paired]\n",
        "\n",
        "        print(f\"[EyesDataset] Total samples: {len(self.file_paths)}\")\n",
        "        if len(self.file_paths) > 0:\n",
        "            print(f\"[EyesDataset] Real: {self.labels.count(0)}, Fake: {self.labels.count(1)}\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.file_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.file_paths[idx]\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        img_bgr = cv2.imread(img_path)\n",
        "        img_tensor = eyes_image_transform(img_bgr, self.img_size)\n",
        "\n",
        "        label_tensor = torch.tensor(label, dtype=torch.long)\n",
        "        return img_tensor, label_tensor, str(img_path)\n",
        "\n",
        "\n",
        "def create_eyes_dataloader(\n",
        "    data_root,\n",
        "    img_size=50,\n",
        "    batch_size=32,\n",
        "    shuffle=False,\n",
        "    num_workers=2,\n",
        "    pin_memory=True,\n",
        "):\n",
        "    \"\"\"\n",
        "    Build eyes dataloader for evaluation.\n",
        "    \"\"\"\n",
        "    dataset = EyesDataset(data_root=data_root, img_size=img_size)\n",
        "    loader = DataLoader(\n",
        "        dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=shuffle,\n",
        "        num_workers=num_workers,\n",
        "        pin_memory=pin_memory,\n",
        "    )\n",
        "    return dataset, loader\n"
      ],
      "metadata": {
        "id": "XPAuBvj1L3bH"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# FACE TRANSFORM\n",
        "face_val_transform = transforms.Compose([\n",
        "    transforms.Resize((FACE_IMG_SIZE, FACE_IMG_SIZE)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                         std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "\n",
        "class FaceDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Full-face dataset for deepfake detection.\n",
        "    Directory structure:\n",
        "        REAL_FACE_DIR: /.../CroppedFaces/original\n",
        "        FAKE_FACE_DIR: /.../CroppedFaces/manipulated\n",
        "    Returns (image_tensor, label_tensor, path_str).\n",
        "    \"\"\"\n",
        "    def __init__(self, real_dir, fake_dir, transform=None):\n",
        "        self.real_dir = Path(real_dir)\n",
        "        self.fake_dir = Path(fake_dir)\n",
        "        self.transform = transform\n",
        "\n",
        "        self.samples = []\n",
        "\n",
        "        # Real label = 0\n",
        "        if self.real_dir.exists():\n",
        "            for ext in [\"*.jpg\", \"*.png\", \"*.jpeg\"]:\n",
        "                for p in self.real_dir.glob(ext):\n",
        "                    self.samples.append((p, 0))\n",
        "\n",
        "        # Fake label = 1\n",
        "        if self.fake_dir.exists():\n",
        "            for ext in [\"*.jpg\", \"*.png\", \"*.jpeg\"]:\n",
        "                for p in self.fake_dir.glob(ext):\n",
        "                    self.samples.append((p, 1))\n",
        "\n",
        "        # Sort for deterministic order\n",
        "        self.samples.sort(key=lambda x: str(x[0]))\n",
        "\n",
        "        num_real = sum(1 for _, lbl in self.samples if lbl == 0)\n",
        "        num_fake = sum(1 for _, lbl in self.samples if lbl == 1)\n",
        "\n",
        "        print(f\"[FaceDataset] Total samples: {len(self.samples)}\")\n",
        "        print(f\"[FaceDataset] Real: {num_real}, Fake: {num_fake}\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path, label = self.samples[idx]\n",
        "\n",
        "        try:\n",
        "            img = Image.open(img_path).convert(\"RGB\")\n",
        "        except Exception as e:\n",
        "            print(f\"[FaceDataset] Error loading {img_path}: {e}\")\n",
        "            img = Image.new(\"RGB\", (FACE_IMG_SIZE, FACE_IMG_SIZE), color=\"black\")\n",
        "\n",
        "        if self.transform is not None:\n",
        "            img = self.transform(img)\n",
        "\n",
        "        label_tensor = torch.tensor(label, dtype=torch.long)\n",
        "        return img, label_tensor, str(img_path)\n",
        "\n",
        "\n",
        "def create_face_dataloader(\n",
        "    real_dir,\n",
        "    fake_dir,\n",
        "    img_size=224,\n",
        "    batch_size=64,\n",
        "    shuffle=False,\n",
        "    num_workers=2,\n",
        "    pin_memory=True,\n",
        "):\n",
        "    \"\"\"\n",
        "    Build dataloader for full-face model evaluation.\n",
        "    \"\"\"\n",
        "    # If you want img_size to be configurable, you can rebuild the transform here.\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize((img_size, img_size)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                             std=[0.229, 0.224, 0.225])\n",
        "    ])\n",
        "\n",
        "    dataset = FaceDataset(real_dir=real_dir, fake_dir=fake_dir, transform=transform)\n",
        "\n",
        "    loader = DataLoader(\n",
        "        dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=shuffle,\n",
        "        num_workers=num_workers,\n",
        "        pin_memory=pin_memory,\n",
        "    )\n",
        "    return dataset, loader"
      ],
      "metadata": {
        "id": "2xskJycvSlw1"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# MOUTH MODEL\n",
        "class Model_mouth(nn.Module):\n",
        "    def __init__(self, num_classes=2):\n",
        "        super(Model_mouth, self).__init__()\n",
        "\n",
        "        self.features = nn.Sequential(\n",
        "            # ---- Block 1 ----\n",
        "            nn.Conv2d(3, 32, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.BatchNorm2d(32),\n",
        "\n",
        "            nn.Conv2d(32, 32, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.BatchNorm2d(32),\n",
        "\n",
        "            nn.Conv2d(32, 32, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.BatchNorm2d(32),\n",
        "\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),  # 64->32\n",
        "            nn.Dropout(0.3),\n",
        "\n",
        "            # ---- Block 2 ----\n",
        "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.BatchNorm2d(64),\n",
        "\n",
        "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.BatchNorm2d(64),\n",
        "\n",
        "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.BatchNorm2d(64),\n",
        "\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),  # 32->16\n",
        "            nn.Dropout(0.3),\n",
        "\n",
        "            # ---- Block 3 ----\n",
        "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.BatchNorm2d(128),\n",
        "\n",
        "            nn.Conv2d(128, 128, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.BatchNorm2d(128),\n",
        "\n",
        "            nn.Conv2d(128, 128, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.BatchNorm2d(128),\n",
        "\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),  # 16->8\n",
        "            nn.Dropout(0.3),\n",
        "        )\n",
        "\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(128 * 8 * 8, 512),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(512, num_classes),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = self.classifier(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "def load_mouth_model(weights_path, device, num_classes=2):\n",
        "    model = Model_mouth(num_classes=num_classes).to(device)\n",
        "    ckpt = torch.load(weights_path, map_location=device)\n",
        "\n",
        "    if \"model_state_dict\" in ckpt:\n",
        "        state_dict = ckpt[\"model_state_dict\"]\n",
        "    else:\n",
        "        state_dict = ckpt\n",
        "\n",
        "    model.load_state_dict(state_dict)\n",
        "    model.eval()\n",
        "    print(f\"[load_mouth_model] Loaded weights from: {weights_path}\")\n",
        "    return model"
      ],
      "metadata": {
        "id": "HZUZ1IeCGCOo"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# EYES MODEL\n",
        "\n",
        "class Model_eyes(nn.Module):\n",
        "    \"\"\"\n",
        "    Same architecture as ModelA in training code.\n",
        "    Input: 50x50x3\n",
        "    \"\"\"\n",
        "    def __init__(self, num_classes=2):\n",
        "        super(Model_eyes, self).__init__()\n",
        "\n",
        "        # Block 1\n",
        "        self.block1 = nn.Sequential(\n",
        "            nn.Conv2d(3, 32, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(32, 32, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2, 2),\n",
        "            nn.Dropout(0.3),\n",
        "        )\n",
        "\n",
        "        # Block 2\n",
        "        self.block2 = nn.Sequential(\n",
        "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2, 2),\n",
        "            nn.Dropout(0.3),\n",
        "        )\n",
        "\n",
        "        # Block 3\n",
        "        self.block3 = nn.Sequential(\n",
        "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(128, 128, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2, 2),\n",
        "            nn.Dropout(0.3),\n",
        "        )\n",
        "\n",
        "        # Classifier\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(128 * 6 * 6, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(512, num_classes),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.block1(x)\n",
        "        x = self.block2(x)\n",
        "        x = self.block3(x)\n",
        "        x = self.classifier(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "def load_eyes_model(weights_path, device, num_classes=2):\n",
        "    \"\"\"\n",
        "    Load eyes model trained with ModelA architecture.\n",
        "    The training script saved with: torch.save(model.state_dict(), ...)\n",
        "    \"\"\"\n",
        "    model = Model_eyes(num_classes=num_classes).to(device)\n",
        "    state_dict = torch.load(weights_path, map_location=device)\n",
        "    model.load_state_dict(state_dict)\n",
        "    model.eval()\n",
        "    print(f\"[load_eyes_model] Loaded weights from: {weights_path}\")\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "CZOAWDx2Mcpz"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# MODEL ARCHITECTURE - PAPER'S MODEL C\n",
        "# ============================================================================\n",
        "\n",
        "class MultiHeadSelfAttention(nn.Module):\n",
        "    \"\"\"Multi-Head Self-Attention mechanism\"\"\"\n",
        "    def __init__(self, embed_dim: int = 1024, num_heads: int = 8, dropout: float = 0.1):\n",
        "        super().__init__()\n",
        "        self.embed_dim = embed_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = embed_dim // num_heads\n",
        "\n",
        "        assert self.head_dim * num_heads == embed_dim, \"embed_dim must be divisible by num_heads\"\n",
        "\n",
        "        self.qkv = nn.Linear(embed_dim, embed_dim * 3)\n",
        "        self.proj = nn.Linear(embed_dim, embed_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        B, N, C = x.shape\n",
        "\n",
        "        # Generate Q, K, V\n",
        "        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, self.head_dim).permute(2, 0, 3, 1, 4)\n",
        "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
        "\n",
        "        # Scaled dot-product attention\n",
        "        attn = (q @ k.transpose(-2, -1)) * (self.head_dim ** -0.5)\n",
        "        attn = attn.softmax(dim=-1)\n",
        "        attn = self.dropout(attn)\n",
        "\n",
        "        # Apply attention to values\n",
        "        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
        "        x = self.proj(x)\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "class TransformerBlock(nn.Module):\n",
        "    \"\"\"Transformer encoder block\"\"\"\n",
        "    def __init__(self, dim: int, heads: int, mlp_dim: int, dropout: float = 0.1):\n",
        "        super().__init__()\n",
        "        self.norm1 = nn.LayerNorm(dim)\n",
        "        self.attn = MultiHeadSelfAttention(dim, heads, dropout)\n",
        "        self.norm2 = nn.LayerNorm(dim)\n",
        "\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(dim, mlp_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(mlp_dim, dim),\n",
        "            nn.Dropout(dropout)\n",
        "        )\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        # Pre-norm architecture\n",
        "        x = x + self.attn(self.norm1(x))\n",
        "        x = x + self.mlp(self.norm2(x))\n",
        "        return x\n",
        "\n",
        "\n",
        "class SimpleCNNModule(nn.Module):\n",
        "    \"\"\"\n",
        "    Simple CNN Module as per paper's Model C\n",
        "    Two Conv2D layers (32 filters, 3x3 kernel) + Max Pooling\n",
        "    \"\"\"\n",
        "    def __init__(self, in_channels=3, out_channels=32):\n",
        "        super().__init__()\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        )\n",
        "        self.out_channels = out_channels\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        return self.features(x)\n",
        "\n",
        "\n",
        "class PaperViTModule(nn.Module):\n",
        "    \"\"\"\n",
        "    Vision Transformer Module - processes CNN feature maps\n",
        "    \"\"\"\n",
        "    def __init__(self, feature_size: int, patch_size: int = 7,\n",
        "                 dim: int = 1024, depth: int = 6, heads: int = 8,\n",
        "                 mlp_dim: int = 2048, dropout: float = 0.1):\n",
        "        super().__init__()\n",
        "        self.feature_size = feature_size\n",
        "        self.patch_size = patch_size\n",
        "        self.dim = dim\n",
        "        self.depth = depth\n",
        "\n",
        "        # Calculate number of patches\n",
        "        self.n_patches = (feature_size // patch_size) ** 2\n",
        "\n",
        "        # Patch embedding: convert patches to vectors\n",
        "        self.patch_embed = nn.Conv2d(\n",
        "            32, dim, kernel_size=patch_size, stride=patch_size\n",
        "        )\n",
        "\n",
        "        # Position embeddings\n",
        "        self.pos_embed = nn.Parameter(torch.zeros(1, self.n_patches, dim))\n",
        "\n",
        "        # Transformer encoder blocks\n",
        "        self.blocks = nn.ModuleList([\n",
        "            TransformerBlock(dim, heads, mlp_dim, dropout)\n",
        "            for _ in range(depth)\n",
        "        ])\n",
        "\n",
        "        self.norm = nn.LayerNorm(dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        # Initialize weights\n",
        "        nn.init.trunc_normal_(self.pos_embed, std=0.02)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        B = x.shape[0]\n",
        "\n",
        "        # Patch embedding: (B, 32, H, W) -> (B, dim, H', W')\n",
        "        x = self.patch_embed(x)\n",
        "\n",
        "        # Flatten spatial dimensions: (B, dim, H', W') -> (B, n_patches, dim)\n",
        "        B, C, H, W = x.shape\n",
        "        x = x.flatten(2).transpose(1, 2)  # (B, n_patches, dim)\n",
        "\n",
        "        # Add position embedding\n",
        "        x = x + self.pos_embed\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        # Apply transformer blocks\n",
        "        for block in self.blocks:\n",
        "            x = block(x)\n",
        "\n",
        "        x = self.norm(x)\n",
        "\n",
        "        return x  # (B, n_patches, dim)\n",
        "\n",
        "\n",
        "class PaperModelC(nn.Module):\n",
        "    \"\"\"\n",
        "    Model C from Paper: CNN-ViT for Full Face Detection\n",
        "    Architecture matches the paper exactly\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        img_size: int = 224,\n",
        "        num_classes: int = 2,\n",
        "        patch_size: int = 7,\n",
        "        dim: int = 1024,\n",
        "        depth: int = 6,\n",
        "        heads: int = 8,\n",
        "        mlp_dim: int = 2048,\n",
        "        dropout: float = 0.1\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        self.img_size = img_size\n",
        "\n",
        "        # CNN Module (as per paper)\n",
        "        self.cnn_module = SimpleCNNModule(in_channels=3, out_channels=32)\n",
        "\n",
        "        # Calculate feature size after CNN (224 -> 112 after MaxPool)\n",
        "        feature_size = img_size // 2\n",
        "\n",
        "        # ViT Module (processes CNN features)\n",
        "        self.vit_module = PaperViTModule(\n",
        "            feature_size=feature_size,\n",
        "            patch_size=patch_size,\n",
        "            dim=dim,\n",
        "            depth=depth,\n",
        "            heads=heads,\n",
        "            mlp_dim=mlp_dim,\n",
        "            dropout=dropout\n",
        "        )\n",
        "\n",
        "        # Global Average Pooling (as per paper)\n",
        "        self.global_pool = nn.AdaptiveAvgPool1d(1)\n",
        "\n",
        "        # Classification Head (Dense layer with Softmax)\n",
        "        self.classifier = nn.Linear(dim, num_classes)\n",
        "\n",
        "        # Initialize weights\n",
        "        self._initialize_weights()\n",
        "\n",
        "    def _initialize_weights(self):\n",
        "        \"\"\"Initialize weights\"\"\"\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Linear):\n",
        "                nn.init.trunc_normal_(m.weight, std=0.02)\n",
        "                if m.bias is not None:\n",
        "                    nn.init.constant_(m.bias, 0)\n",
        "            elif isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        # CNN feature extraction: (B, 3, 224, 224) -> (B, 32, 112, 112)\n",
        "        cnn_features = self.cnn_module(x)\n",
        "\n",
        "        # ViT processing: (B, 32, 112, 112) -> (B, n_patches, dim)\n",
        "        vit_features = self.vit_module(cnn_features)  # (B, n_patches, dim)\n",
        "\n",
        "        # Global Average Pooling: (B, n_patches, dim) -> (B, dim)\n",
        "        vit_features = vit_features.transpose(1, 2)  # (B, dim, n_patches)\n",
        "        pooled = self.global_pool(vit_features).squeeze(-1)  # (B, dim)\n",
        "\n",
        "        # Classification: (B, dim) -> (B, num_classes)\n",
        "        logits = self.classifier(pooled)\n",
        "\n",
        "        return logits\n",
        "\n",
        "print(\"✓ Model architecture defined successfully!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UBZZqZFGTGvE",
        "outputId": "4e7d7ed4-76d1-4e24-f0eb-924511be5711"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ Model architecture defined successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============ FACE MODEL LOADING ============\n",
        "\n",
        "def load_face_model(weights_path, device, num_classes=2):\n",
        "    \"\"\"\n",
        "    Load full-face model (PaperModelC) from checkpoint.\n",
        "    \"\"\"\n",
        "    # Must match training-time hyperparameters\n",
        "    model = PaperModelC(\n",
        "        img_size=224,\n",
        "        num_classes=num_classes,\n",
        "        patch_size=7,\n",
        "        dim=1024,\n",
        "        depth=6,\n",
        "        heads=8,\n",
        "        mlp_dim=2048,\n",
        "        dropout=0.1,   # same as in your training code\n",
        "    ).to(device)\n",
        "\n",
        "    ckpt = torch.load(weights_path, map_location=device)\n",
        "\n",
        "    if isinstance(ckpt, dict) and \"model_state_dict\" in ckpt:\n",
        "        state_dict = ckpt[\"model_state_dict\"]\n",
        "        print(\"[load_face_model] Detected checkpoint dict with 'model_state_dict'.\")\n",
        "    else:\n",
        "        state_dict = ckpt\n",
        "        print(\"[load_face_model] Detected raw state_dict.\")\n",
        "\n",
        "    model.load_state_dict(state_dict)\n",
        "    model.eval()\n",
        "    print(f\"[load_face_model] Loaded weights from: {weights_path}\")\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "nWtEhjAAStU3"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# MOUTH MODEL LOADING & INFERENCE\n",
        "\n",
        "def predict_mouth_batch(model, imgs, device):\n",
        "    model.eval()\n",
        "    imgs = imgs.to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        logits = model(imgs)\n",
        "        probs = F.softmax(logits, dim=1)\n",
        "        preds = torch.argmax(probs, dim=1)\n",
        "\n",
        "    return logits, probs, preds\n",
        "\n",
        "\n",
        "def evaluate_mouth_model(model, dataloader, device, verbose=True):\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    model.eval()\n",
        "\n",
        "    total_loss = 0.0\n",
        "    total_correct = 0\n",
        "    total_samples = 0\n",
        "\n",
        "    all_results = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for imgs, labels, paths in dataloader:\n",
        "            imgs = imgs.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            logits = model(imgs)\n",
        "            loss = criterion(logits, labels)\n",
        "\n",
        "            probs = F.softmax(logits, dim=1)\n",
        "            preds = torch.argmax(probs, dim=1)\n",
        "\n",
        "            total_loss += loss.item() * imgs.size(0)\n",
        "            total_correct += (preds == labels).sum().item()\n",
        "            total_samples += labels.size(0)\n",
        "\n",
        "            # Save per-sample results\n",
        "            probs_np = probs.cpu().numpy()\n",
        "            preds_np = preds.cpu().numpy()\n",
        "            labels_np = labels.cpu().numpy()\n",
        "\n",
        "            for pth, y_true, y_pred, prob_vec in zip(\n",
        "                paths, labels_np, preds_np, probs_np\n",
        "            ):\n",
        "                # Assuming class 0 = real, 1 = fake\n",
        "                all_results.append(\n",
        "                    {\n",
        "                        \"path\": pth,\n",
        "                        \"label\": int(y_true),\n",
        "                        \"pred\": int(y_pred),\n",
        "                        \"prob_real\": float(prob_vec[0]),\n",
        "                        \"prob_fake\": float(prob_vec[1]),\n",
        "                    }\n",
        "                )\n",
        "\n",
        "    avg_loss = total_loss / max(1, total_samples)\n",
        "    acc = total_correct / max(1, total_samples)\n",
        "\n",
        "    if verbose:\n",
        "        print(f\"[Mouth] Eval Loss: {avg_loss:.4f} | Eval Acc: {acc:.4f} \"\n",
        "              f\"({total_correct}/{total_samples})\")\n",
        "\n",
        "    return acc, all_results"
      ],
      "metadata": {
        "id": "u5w3cD5uG3Ya"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# EYES INFERENCE & EVALUATION\n",
        "\n",
        "def predict_eyes_batch(model, imgs, device):\n",
        "    \"\"\"\n",
        "    Run one forward pass on a batch of eye crops.\n",
        "    Returns logits, probabilities, and predicted labels.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    imgs = imgs.to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        logits = model(imgs)\n",
        "        probs = torch.softmax(logits, dim=1)\n",
        "        preds = torch.argmax(probs, dim=1)\n",
        "\n",
        "    return logits, probs, preds\n",
        "\n",
        "\n",
        "def evaluate_eyes_model(model, dataloader, device, verbose=True):\n",
        "    \"\"\"\n",
        "    Evaluate eyes model on the given dataloader.\n",
        "    Returns:\n",
        "        overall_acc, results_list\n",
        "    results_list elements:\n",
        "        {\n",
        "            \"path\": img_path,\n",
        "            \"label\": int_label,\n",
        "            \"pred\": int_pred,\n",
        "            \"prob_real\": float,\n",
        "            \"prob_fake\": float,\n",
        "        }\n",
        "    \"\"\"\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    model.eval()\n",
        "\n",
        "    total_loss = 0.0\n",
        "    total_correct = 0\n",
        "    total_samples = 0\n",
        "\n",
        "    all_results = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for imgs, labels, paths in dataloader:\n",
        "            imgs = imgs.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            logits = model(imgs)\n",
        "            loss = criterion(logits, labels)\n",
        "\n",
        "            probs = torch.softmax(logits, dim=1)\n",
        "            preds = torch.argmax(probs, dim=1)\n",
        "\n",
        "            total_loss += loss.item() * imgs.size(0)\n",
        "            total_correct += (preds == labels).sum().item()\n",
        "            total_samples += labels.size(0)\n",
        "\n",
        "            probs_np = probs.cpu().numpy()\n",
        "            preds_np = preds.cpu().numpy()\n",
        "            labels_np = labels.cpu().numpy()\n",
        "\n",
        "            for pth, y_true, y_pred, prob_vec in zip(paths, labels_np, preds_np, probs_np):\n",
        "                all_results.append(\n",
        "                    {\n",
        "                        \"path\": pth,\n",
        "                        \"label\": int(y_true),\n",
        "                        \"pred\": int(y_pred),\n",
        "                        \"prob_real\": float(prob_vec[0]),\n",
        "                        \"prob_fake\": float(prob_vec[1]),\n",
        "                    }\n",
        "                )\n",
        "\n",
        "    avg_loss = total_loss / max(1, total_samples)\n",
        "    acc = total_correct / max(1, total_samples)\n",
        "\n",
        "    if verbose:\n",
        "        print(f\"[Eyes] Eval Loss: {avg_loss:.4f} | Eval Acc: {acc:.4f} \"\n",
        "              f\"({total_correct}/{total_samples})\")\n",
        "\n",
        "    return acc, all_results\n"
      ],
      "metadata": {
        "id": "F4oVF2SAMEus"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============ FACE INFERENCE & EVALUATION ============\n",
        "\n",
        "def predict_face_batch(model, imgs, device):\n",
        "    \"\"\"\n",
        "    Run one forward pass on a batch of full-face images.\n",
        "    Returns:\n",
        "        logits, probs, preds\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    imgs = imgs.to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        logits = model(imgs)\n",
        "        probs = F.softmax(logits, dim=1)\n",
        "        preds = torch.argmax(probs, dim=1)\n",
        "\n",
        "    return logits, probs, preds\n",
        "\n",
        "\n",
        "def evaluate_face_model(model, dataloader, device, verbose=True):\n",
        "    \"\"\"\n",
        "    Evaluate face model on the given dataloader.\n",
        "    Returns:\n",
        "        overall_acc, results_list\n",
        "    results_list elements:\n",
        "        {\n",
        "            \"path\": img_path,\n",
        "            \"label\": int_label,\n",
        "            \"pred\": int_pred,\n",
        "            \"prob_real\": float,\n",
        "            \"prob_fake\": float,\n",
        "        }\n",
        "    \"\"\"\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    model.eval()\n",
        "\n",
        "    total_loss = 0.0\n",
        "    total_correct = 0\n",
        "    total_samples = 0\n",
        "\n",
        "    all_results = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for imgs, labels, paths in dataloader:\n",
        "            imgs = imgs.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            logits = model(imgs)\n",
        "            loss = criterion(logits, labels)\n",
        "\n",
        "            probs = F.softmax(logits, dim=1)\n",
        "            preds = torch.argmax(probs, dim=1)\n",
        "\n",
        "            total_loss += loss.item() * imgs.size(0)\n",
        "            total_correct += (preds == labels).sum().item()\n",
        "            total_samples += labels.size(0)\n",
        "\n",
        "            probs_np = probs.cpu().numpy()\n",
        "            preds_np = preds.cpu().numpy()\n",
        "            labels_np = labels.cpu().numpy()\n",
        "\n",
        "            for pth, y_true, y_pred, prob_vec in zip(paths, labels_np, preds_np, probs_np):\n",
        "                all_results.append(\n",
        "                    {\n",
        "                        \"path\": pth,\n",
        "                        \"label\": int(y_true),\n",
        "                        \"pred\": int(y_pred),\n",
        "                        \"prob_real\": float(prob_vec[0]),\n",
        "                        \"prob_fake\": float(prob_vec[1]),\n",
        "                    }\n",
        "                )\n",
        "\n",
        "    avg_loss = total_loss / max(1, total_samples)\n",
        "    acc = total_correct / max(1, total_samples)\n",
        "\n",
        "    if verbose:\n",
        "        print(f\"[Face] Eval Loss: {avg_loss:.4f} | Eval Acc: {acc:.4f} \"\n",
        "              f\"({total_correct}/{total_samples})\")\n",
        "\n",
        "    return acc, all_results\n"
      ],
      "metadata": {
        "id": "ZIlU58SpS6bW"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# RUN MOUTH EVALUATION\n",
        "\n",
        "mouth_dataset, mouth_loader = create_mouth_dataloader(\n",
        "    real_dir=REAL_MOUTH_DIR,\n",
        "    fake_dir=FAKE_MOUTH_DIR,\n",
        "    img_size=MOUTH_IMG_SIZE,\n",
        "    batch_size=MOPUTH_BATCH_SIZE,\n",
        "    shuffle=False,\n",
        "    num_workers=2,\n",
        "    pin_memory=True,\n",
        ")\n",
        "\n",
        "mouth_model = load_mouth_model(MOUTH_MODEL_WEIGHTS, device=device, num_classes=2)\n",
        "mouth_acc, mouth_results = evaluate_mouth_model(\n",
        "    mouth_model,\n",
        "    mouth_loader,\n",
        "    device=device,\n",
        "    verbose=True,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pja7hDxjJHf2",
        "outputId": "633857ad-e669-4363-ab36-8a467befc446"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[MouthDataset] Total samples: 24393\n",
            "[load_mouth_model] Loaded weights from: /content/drive/MyDrive/Colab Notebooks/EECE7370/Model_mouth/CNN_Mouth_best_1207_0909.pth\n",
            "[Mouth] Eval Loss: 0.0327 | Eval Acc: 0.9906 (24163/24393)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# RUN EYES EVALUATION\n",
        "\n",
        "eyes_dataset, eyes_loader = create_eyes_dataloader(\n",
        "    data_root=EYES_DATA_ROOT,\n",
        "    img_size=EYES_IMG_SIZE,\n",
        "    batch_size=EYES_BATCH_SIZE,\n",
        "    shuffle=False,\n",
        "    num_workers=2,\n",
        "    pin_memory=True,\n",
        ")\n",
        "\n",
        "eyes_model = load_eyes_model(\n",
        "    weights_path=EYES_MODEL_WEIGHTS,\n",
        "    device=device,\n",
        "    num_classes=2,\n",
        ")\n",
        "\n",
        "eyes_acc, eyes_results = evaluate_eyes_model(\n",
        "    eyes_model,\n",
        "    eyes_loader,\n",
        "    device=device,\n",
        "    verbose=True,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NOw48dSCMQJy",
        "outputId": "17f13647-9056-4f40-b5db-d0c712e17588"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[EyesDataset] Total samples: 48786\n",
            "[EyesDataset] Real: 24398, Fake: 24388\n",
            "[load_eyes_model] Loaded weights from: /content/drive/MyDrive/Colab Notebooks/EECE7370/Model_eyes_nose/model_a_eyes_best.pth\n",
            "[Eyes] Eval Loss: 0.0312 | Eval Acc: 0.9860 (48104/48786)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# RUN FACE EVALUATION\n",
        "\n",
        "face_dataset, face_loader = create_face_dataloader(\n",
        "    real_dir=REAL_FACE_DIR,\n",
        "    fake_dir=FAKE_FACE_DIR,\n",
        "    img_size=FACE_IMG_SIZE,\n",
        "    batch_size=FACE_BATCH_SIZE,\n",
        "    shuffle=False,\n",
        "    num_workers=2,\n",
        "    pin_memory=True,\n",
        ")\n",
        "\n",
        "face_model = load_face_model(\n",
        "    weights_path=FACE_MODEL_WEIGHTS,\n",
        "    device=device,\n",
        "    num_classes=2,\n",
        ")\n",
        "\n",
        "face_acc, face_results = evaluate_face_model(\n",
        "    face_model,\n",
        "    face_loader,\n",
        "    device=device,\n",
        "    verbose=True,\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JaMIiaL_TJpK",
        "outputId": "b19b86e2-85ef-4693-9be9-fb91e5357321"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[FaceDataset] Total samples: 26455\n",
            "[FaceDataset] Real: 13109, Fake: 13346\n",
            "[load_face_model] Detected checkpoint dict with 'model_state_dict'.\n",
            "[load_face_model] Loaded weights from: /content/drive/MyDrive/Colab Notebooks/EECE7370/Model_face/paper_model_c_best.pth\n",
            "[Face] Eval Loss: 0.1401 | Eval Acc: 0.9503 (25140/26455)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Using Mouth Model:\")\n",
        "print(f\"[Mouth] Overall accuracy on mouth dataset: {mouth_acc:.4f}\")\n",
        "print(\"Example result:\", mouth_results[0] if len(mouth_results) > 0 else \"No samples\")\n",
        "\n",
        "print(\"Using Eyes Model:\")\n",
        "print(f\"[Eyes] Overall accuracy on eyes dataset: {eyes_acc:.4f}\")\n",
        "print(\"Example result:\", eyes_results[0] if len(eyes_results) > 0 else \"No samples\")\n",
        "\n",
        "print(\"Using Face Model:\")\n",
        "print(f\"[Face] Overall accuracy on face dataset: {face_acc:.4f}\")\n",
        "print(\"Example result:\", face_results[0] if len(face_results) > 0 else \"No samples\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TkFvRgo6PJhx",
        "outputId": "4bd4ca5b-b3d4-416c-858f-7bf7c926b6fd"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using Mouth Model:\n",
            "[Mouth] Overall accuracy on mouth dataset: 0.9906\n",
            "Example result: {'path': '/content/tmp_extract/Features/manipulated/mouth/manipulated_manipulated_frame0_face0_face0_mouth.jpg', 'label': 1, 'pred': 1, 'prob_real': 1.203316092601714e-12, 'prob_fake': 1.0}\n",
            "Using Eyes Model:\n",
            "[Eyes] Overall accuracy on eyes dataset: 0.9860\n",
            "Example result: {'path': '/content/tmp_extract/Features/manipulated/leftEye/manipulated_manipulated_frame0_face0_face0_left_eye.jpg', 'label': 1, 'pred': 1, 'prob_real': 9.361333468405064e-07, 'prob_fake': 0.9999990463256836}\n",
            "Using Face Model:\n",
            "[Face] Overall accuracy on face dataset: 0.9503\n",
            "Example result: {'path': '/content/tmp_extract/CroppedFaces/manipulated/manipulated_frame0_face0.jpg', 'label': 1, 'pred': 1, 'prob_real': 1.843179688876262e-07, 'prob_fake': 0.9999997615814209}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_model_weights(mouth_acc, eyes_acc, face_acc, eps=1e-6):\n",
        "    accs = np.array([mouth_acc, eyes_acc, face_acc], dtype=np.float32)\n",
        "    accs = np.maximum(accs, eps)          # avoid zeros\n",
        "    acc_sum = float(accs.sum())\n",
        "\n",
        "    w_mouth = float(accs[0] / acc_sum)\n",
        "    w_eyes  = float(accs[1] / acc_sum)\n",
        "    w_face  = float(accs[2] / acc_sum)\n",
        "\n",
        "    weights = {\n",
        "        \"mouth\": w_mouth,\n",
        "        \"eyes\":  w_eyes,\n",
        "        \"face\":  w_face,\n",
        "    }\n",
        "\n",
        "    print(\"[Fusion] Computed weights from accuracies:\")\n",
        "    print(f\"  Mouth: {w_mouth:.4f} (acc={mouth_acc:.4f})\")\n",
        "    print(f\"  Eyes : {w_eyes:.4f} (acc={eyes_acc:.4f})\")\n",
        "    print(f\"  Face : {w_face:.4f} (acc={face_acc:.4f})\")\n",
        "\n",
        "    return weights"
      ],
      "metadata": {
        "id": "BLp6fTRIWlWD"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ======================================================================\n",
        "# HELPER: EXTRACT SAMPLE KEY FROM FILENAME\n",
        "# ======================================================================\n",
        "\n",
        "def extract_sample_key(path, region_type):\n",
        "    name = os.path.splitext(os.path.basename(path))[0]\n",
        "    tokens = name.split('_')\n",
        "\n",
        "    # Face filenames are usually like: manipulated_frame10020_face0\n",
        "    if region_type == \"face\":\n",
        "        if len(tokens) >= 3:\n",
        "            # e.g. [\"manipulated\", \"frame10020\", \"face0\"]\n",
        "            return f\"{tokens[0]}_{tokens[1]}_{tokens[2]}\"\n",
        "        return name\n",
        "\n",
        "    # Mouth / Eyes filenames are usually like:\n",
        "    #   manipulated_manipulated_frame10020_face0_face0_left_eye\n",
        "    #   original_original_frame10020_face0_face0_mouth\n",
        "    if region_type in [\"mouth\", \"eyes\"]:\n",
        "        if len(tokens) >= 4 and tokens[0] in [\"original\", \"manipulated\"]:\n",
        "            # Common pattern: [status, status, frameXXXX, faceX, faceX, ...]\n",
        "            # Use tokens[0] + tokens[2] + tokens[3]\n",
        "            if tokens[2].startswith(\"frame\") and tokens[3].startswith(\"face\"):\n",
        "                return f\"{tokens[0]}_{tokens[2]}_{tokens[3]}\"\n",
        "\n",
        "            # Fallback for slightly different patterns:\n",
        "            # Find \"frame...\" and the first \"face...\" after it\n",
        "            frame_idx = None\n",
        "            face_idx = None\n",
        "            for i, t in enumerate(tokens):\n",
        "                if t.startswith(\"frame\"):\n",
        "                    frame_idx = i\n",
        "                if frame_idx is not None and face_idx is None and t.startswith(\"face\") and i > frame_idx:\n",
        "                    face_idx = i\n",
        "                    break\n",
        "\n",
        "            if frame_idx is not None and face_idx is not None:\n",
        "                return f\"{tokens[0]}_{tokens[frame_idx]}_{tokens[face_idx]}\"\n",
        "\n",
        "        # Last fallback: just join the first 3 tokens\n",
        "        if len(tokens) >= 3:\n",
        "            return f\"{tokens[0]}_{tokens[1]}_{tokens[2]}\"\n",
        "        return name\n",
        "\n",
        "    # Default fallback\n",
        "    return name\n"
      ],
      "metadata": {
        "id": "FZoaeqoYZkI1"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ======================================================================\n",
        "# FUNCTION 2 (UPDATED): FUSE MODEL OUTPUTS USING WEIGHTS + SAMPLE KEY\n",
        "# ======================================================================\n",
        "\n",
        "def fuse_model_outputs(mouth_results, eyes_results, face_results, weights):\n",
        "    \"\"\"\n",
        "    Fuse predictions from mouth, eyes, and face models using given weights.\n",
        "    We align samples by a common sample key extracted from the filename,\n",
        "    e.g. \"manipulated_frame10020_face0\".\n",
        "\n",
        "    Args:\n",
        "        mouth_results: list of dicts from evaluate_mouth_model(...)\n",
        "        eyes_results:  list of dicts from evaluate_eyes_model(...)\n",
        "        face_results:  list of dicts from evaluate_face_model(...)\n",
        "        weights:       dict with keys 'mouth', 'eyes', 'face'\n",
        "    Returns:\n",
        "        fusion_acc: float, accuracy of fused model on the intersection set\n",
        "        fused_results: list of dicts:\n",
        "            {\n",
        "                \"key\": sample_key,\n",
        "                \"label\": int,\n",
        "                \"pred\": int,\n",
        "                \"prob_real\": float,\n",
        "                \"prob_fake\": float,\n",
        "                \"mouth_prob_real\": float,\n",
        "                \"eyes_prob_real\": float,\n",
        "                \"face_prob_real\": float,\n",
        "            }\n",
        "    \"\"\"\n",
        "    # Build dicts keyed by sample key\n",
        "    def build_dict(results, region_type):\n",
        "        d = {}\n",
        "        for r in results:\n",
        "            key = extract_sample_key(r[\"path\"], region_type)\n",
        "            # If multiple patches map to the same key (e.g. left_eye / right_eye),\n",
        "            # you could average them. For now we simply keep the last one.\n",
        "            d[key] = r\n",
        "        return d\n",
        "\n",
        "    mouth_dict = build_dict(mouth_results, \"mouth\")\n",
        "    eyes_dict  = build_dict(eyes_results, \"eyes\")\n",
        "    face_dict  = build_dict(face_results, \"face\")\n",
        "\n",
        "    common_keys = set(mouth_dict.keys()) & set(eyes_dict.keys()) & set(face_dict.keys())\n",
        "    common_keys = sorted(list(common_keys))\n",
        "\n",
        "    if len(common_keys) == 0:\n",
        "        print(\"[Fusion] No common keys between mouth/eyes/face results. \"\n",
        "              \"Please check filename patterns.\")\n",
        "        return 0.0, []\n",
        "\n",
        "    print(f\"[Fusion] Number of common samples across all three models: {len(common_keys)}\")\n",
        "\n",
        "    w_mouth = weights[\"mouth\"]\n",
        "    w_eyes  = weights[\"eyes\"]\n",
        "    w_face  = weights[\"face\"]\n",
        "\n",
        "    total = 0\n",
        "    correct = 0\n",
        "    fused_results = []\n",
        "\n",
        "    for key in common_keys:\n",
        "        m = mouth_dict[key]\n",
        "        e = eyes_dict[key]\n",
        "        f = face_dict[key]\n",
        "\n",
        "        # Use label consistency check\n",
        "        labels = [m[\"label\"], e[\"label\"], f[\"label\"]]\n",
        "        if not (labels[0] == labels[1] == labels[2]):\n",
        "            # If labels disagree, skip this sample (or handle differently if you want)\n",
        "            # print(f\"[Fusion] Warning: label mismatch for key {key}, skipping.\")\n",
        "            continue\n",
        "\n",
        "        label = labels[0]\n",
        "\n",
        "        # Weighted sum of probabilities\n",
        "        prob_real = (\n",
        "            w_mouth * m[\"prob_real\"]\n",
        "            + w_eyes * e[\"prob_real\"]\n",
        "            + w_face * f[\"prob_real\"]\n",
        "        )\n",
        "        prob_fake = (\n",
        "            w_mouth * m[\"prob_fake\"]\n",
        "            + w_eyes * e[\"prob_fake\"]\n",
        "            + w_face * f[\"prob_fake\"]\n",
        "        )\n",
        "\n",
        "        pred = 1 if prob_fake > prob_real else 0\n",
        "\n",
        "        total += 1\n",
        "        if pred == label:\n",
        "            correct += 1\n",
        "\n",
        "        fused_results.append(\n",
        "            {\n",
        "                \"key\": key,\n",
        "                \"label\": int(label),\n",
        "                \"pred\": int(pred),\n",
        "                \"prob_real\": float(prob_real),\n",
        "                \"prob_fake\": float(prob_fake),\n",
        "                \"mouth_prob_real\": float(m[\"prob_real\"]),\n",
        "                \"eyes_prob_real\":  float(e[\"prob_real\"]),\n",
        "                \"face_prob_real\":  float(f[\"prob_real\"]),\n",
        "            }\n",
        "        )\n",
        "\n",
        "    if total == 0:\n",
        "        print(\"[Fusion] No valid samples after label consistency check.\")\n",
        "        fusion_acc = 0.0\n",
        "    else:\n",
        "        fusion_acc = correct / total\n",
        "\n",
        "    print(f\"[Fusion] Accuracy: {fusion_acc:.4f} ({correct}/{total})\")\n",
        "\n",
        "    return fusion_acc, fused_results\n"
      ],
      "metadata": {
        "id": "JwYt2e-SWnLj"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ======================================================================\n",
        "# RUN FUSION EVALUATION\n",
        "# ======================================================================\n",
        "\n",
        "fusion_weights = compute_model_weights(\n",
        "    mouth_acc=mouth_acc,\n",
        "    eyes_acc=eyes_acc,\n",
        "    face_acc=face_acc,\n",
        ")\n",
        "\n",
        "fusion_acc, fusion_results = fuse_model_outputs(\n",
        "    mouth_results=mouth_results,\n",
        "    eyes_results=eyes_results,\n",
        "    face_results=face_results,\n",
        "    weights=fusion_weights,\n",
        ")\n",
        "\n",
        "print(\"\\nUsing Fusion Model (Mouth + Eyes + Face):\")\n",
        "print(f\"[Fusion] Overall accuracy on common samples: {fusion_acc:.4f}\")\n",
        "print(\"Example fusion result:\", fusion_results[0] if len(fusion_results) > 0 else \"No fusion samples\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "57h0nX8-Wudi",
        "outputId": "68218f5c-b8f4-417d-de60-738fa0da8ab9"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Fusion] Computed weights from accuracies:\n",
            "  Mouth: 0.3384 (acc=0.9906)\n",
            "  Eyes : 0.3369 (acc=0.9860)\n",
            "  Face : 0.3247 (acc=0.9503)\n",
            "[Fusion] Number of common samples across all three models: 24392\n",
            "[Fusion] Accuracy: 0.9949 (24268/24392)\n",
            "\n",
            "Using Fusion Model (Mouth + Eyes + Face):\n",
            "[Fusion] Overall accuracy on common samples: 0.9949\n",
            "Example fusion result: {'key': 'manipulated_frame0_face0', 'label': 1, 'pred': 1, 'prob_real': 0.003897471010178527, 'prob_fake': 0.9961025038950453, 'mouth_prob_real': 1.203316092601714e-12, 'eyes_prob_real': 0.011569000780582428, 'face_prob_real': 1.843179688876262e-07}\n"
          ]
        }
      ]
    }
  ]
}